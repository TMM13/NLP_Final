{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-6q5mUSBBaX",
        "outputId": "275641f3-4ac5-4799-c99e-82dae82b08bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n",
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA_pZrgUBOVj",
        "outputId": "b4028758-f73e-4d14-c74c-d09a46ac8644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.14 / client 1.6.12)\n",
            "Dataset URL: https://www.kaggle.com/datasets/saurabhshahane/cyberbullying-dataset\n",
            "License(s): Attribution 4.0 International (CC BY 4.0)\n",
            "Downloading cyberbullying-dataset.zip to /content\n",
            " 82% 53.0M/64.9M [00:00<00:00, 108MB/s]\n",
            "100% 64.9M/64.9M [00:00<00:00, 108MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d saurabhshahane/cyberbullying-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tCujU6ADBYd3"
      },
      "outputs": [],
      "source": [
        "!unzip -q cyberbullying-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npDx9-5DBxQG",
        "outputId": "683d80c1-10d3-40f5-e832-a5368e683f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install nltk torch torchtext scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import Counter\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dr5BKjHmDT2h"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def tokenize_and_lemmatize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "10TJwYdCB9fN"
      },
      "outputs": [],
      "source": [
        "racism_df = pd.read_csv('twitter_racism_parsed_dataset.csv')\n",
        "aggression_df = pd.read_csv('aggression_parsed_dataset.csv')\n",
        "attack_df = pd.read_csv('attack_parsed_dataset.csv')\n",
        "kaggle_df = pd.read_csv('kaggle_parsed_dataset.csv')\n",
        "toxicity_df = pd.read_csv('toxicity_parsed_dataset.csv')\n",
        "sexism_df = pd.read_csv('twitter_sexism_parsed_dataset.csv')\n",
        "twitter_df = pd.read_csv('twitter_parsed_dataset.csv')\n",
        "youtube_df = pd.read_csv('youtube_parsed_dataset.csv')\n",
        "\n",
        "racism_df = racism_df.dropna(subset=['oh_label'])\n",
        "aggression_df = aggression_df.dropna(subset=['oh_label'])\n",
        "attack_df = attack_df.dropna(subset=['oh_label'])\n",
        "kaggle_df = kaggle_df.dropna(subset=['oh_label'])\n",
        "toxicity_df = toxicity_df.dropna(subset=['oh_label'])\n",
        "sexism_df = sexism_df.dropna(subset=['oh_label'])\n",
        "twitter_df = twitter_df.dropna(subset=['oh_label'])\n",
        "youtube_df = youtube_df.dropna(subset=['oh_label'])\n",
        "\n",
        "racism_df['Text'] = racism_df['Text'].apply(lambda x: tokenize_and_lemmatize(clean_text(x)))\n",
        "aggression_df['Text'] = aggression_df['Text'].apply(lambda x: tokenize_and_lemmatize(clean_text(x)))\n",
        "attack_df['Text'] = attack_df['Text'].apply(lambda x: tokenize_and_lemmatize(clean_text(x)))\n",
        "kaggle_df['Text'] = kaggle_df['Text'].apply(lambda x: tokenize_and_lemmatize(clean_text(x)))\n",
        "toxicity_df['Text'] = toxicity_df['Text'].apply(lambda x: tokenize_and_lemmatize(clean_text(x)))\n",
        "sexism_df['Text'] = sexism_df['Text'].apply(lambda x: tokenize_and_lemmatize(clean_text(x)))\n",
        "twitter_df['Text'] = twitter_df['Text'].apply(lambda x: tokenize_and_lemmatize(clean_text(x)))\n",
        "youtube_df['Text'] = youtube_df['Text'].apply(lambda x: tokenize_and_lemmatize(clean_text(x)))\n",
        "\n",
        "racism_df = racism_df[['Text', 'oh_label']]\n",
        "aggression_df = aggression_df[['Text', 'oh_label']]\n",
        "attack_df = attack_df[['Text', 'oh_label']]\n",
        "kaggle_df = kaggle_df[['Text', 'oh_label']]\n",
        "toxicity_df = toxicity_df[['Text', 'oh_label']]\n",
        "sexism_df = sexism_df[['Text', 'oh_label']]\n",
        "twitter_df = twitter_df[['Text', 'oh_label']]\n",
        "youtube_df = youtube_df[['Text', 'oh_label']]\n",
        "\n",
        "combined_df = pd.concat([racism_df, aggression_df, attack_df, kaggle_df, toxicity_df, sexism_df, twitter_df, youtube_df])\n",
        "combined_df.to_csv('combined_cyberbullying_dataset.csv', index=False)\n",
        "combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
        "train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CpXvIlwADY6g"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train_df['Text'])\n",
        "y_train = train_df['oh_label']\n",
        "X_test = vectorizer.transform(test_df['Text'])\n",
        "y_test = test_df['oh_label']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYL5wEjwEIm-",
        "outputId": "12e2eaa0-0f8d-44de-899c-d67d332f506a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes:\n",
            "  Accuracy: 0.9192\n",
            "  Precision: 0.8033\n",
            "  Recall: 0.4795\n",
            "  F1 Score: 0.6005\n",
            "\n",
            "Logistic Regression:\n",
            "  Accuracy: 0.9296\n",
            "  Precision: 0.8131\n",
            "  Recall: 0.5770\n",
            "  F1 Score: 0.6750\n",
            "\n",
            "SVM:\n",
            "  Accuracy: 0.9303\n",
            "  Precision: 0.8205\n",
            "  Recall: 0.5757\n",
            "  F1 Score: 0.6766\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def train_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='binary')\n",
        "    recall = recall_score(y_test, y_pred, average='binary')\n",
        "    f1 = f1_score(y_test, y_pred, average='binary')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "models = {\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"SVM\": SVC(kernel='linear')\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    accuracy, precision, recall, f1 = train_evaluate_model(model, X_train, y_train, X_test, y_test)\n",
        "    results[model_name] = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1 Score\": f1\n",
        "    }\n",
        "\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"  Accuracy: {metrics['Accuracy']:.4f}\")\n",
        "    print(f\"  Precision: {metrics['Precision']:.4f}\")\n",
        "    print(f\"  Recall: {metrics['Recall']:.4f}\")\n",
        "    print(f\"  F1 Score: {metrics['F1 Score']:.4f}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78YXFxcfmGXM",
        "outputId": "d8a6bfe6-87e2-437a-ad51-cd594a9d1efd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(models[\"Naive Bayes\"], 'naive_bayes_model.joblib')\n",
        "joblib.dump(models[\"Logistic Regression\"], 'logistic_regression_model.joblib')\n",
        "joblib.dump(models[\"SVM\"], 'svm_model.joblib')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BVIUKbsXl9DD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85e3ff97-9fd5-4eb1-d056-4fadeeab6d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction using Naive Bayes: Cyberbullying\n"
          ]
        }
      ],
      "source": [
        "def load_models_and_vectorizer():\n",
        "    naive_bayes_model = joblib.load('naive_bayes_model.joblib')\n",
        "    logistic_regression_model = joblib.load('logistic_regression_model.joblib')\n",
        "    svm_model = joblib.load('svm_model.joblib')\n",
        "    vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
        "    return naive_bayes_model, logistic_regression_model, svm_model, vectorizer\n",
        "\n",
        "def predict_cyberbullying(text, model_name):\n",
        "    naive_bayes_model, logistic_regression_model, svm_model, vectorizer = load_models_and_vectorizer()\n",
        "    models = {\n",
        "        \"Naive Bayes\": naive_bayes_model,\n",
        "        \"Logistic Regression\": logistic_regression_model,\n",
        "        \"SVM\": svm_model\n",
        "    }\n",
        "\n",
        "    model = models.get(model_name)\n",
        "    if not model:\n",
        "        raise ValueError(f\"Model '{model_name}' not found. Available models are: {list(models.keys())}\")\n",
        "\n",
        "    text_cleaned = tokenize_and_lemmatize(clean_text(text))\n",
        "    text_vectorized = vectorizer.transform([text_cleaned])\n",
        "    prediction = model.predict(text_vectorized)\n",
        "\n",
        "    return \"Cyberbullying\" if prediction[0] == 1 else \"Not Cyberbullying\"\n",
        "\n",
        "text = \"You are loser you know it haha\"\n",
        "model_name = \"Naive Bayes\"\n",
        "prediction = predict_cyberbullying(text, model_name)\n",
        "print(f\"Prediction using {model_name}: {prediction}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}